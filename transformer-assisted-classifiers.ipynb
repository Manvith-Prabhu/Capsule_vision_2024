{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9619634,"sourceType":"datasetVersion","datasetId":5621305},{"sourceId":10063071,"sourceType":"datasetVersion","datasetId":6201498},{"sourceId":184494,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":157266,"modelId":179683},{"sourceId":186405,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":158914,"modelId":181300},{"sourceId":191143,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":162927,"modelId":185289}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install libraries required","metadata":{}},{"cell_type":"code","source":"!pip install 'git+https://github.com/apple/ml-aim.git#subdirectory=aim-v1'\n!pip install 'git+https://github.com/apple/ml-aim.git#subdirectory=aim-v2'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import Necessary Library functions","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc, recall_score, f1_score, balanced_accuracy_score\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\nimport json\nfrom pathlib import Path\nimport os\nfrom aim.v2.utils import load_pretrained\nfrom aim.v1.torch.data import val_transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:13:28.316485Z","iopub.execute_input":"2025-02-07T14:13:28.317090Z","iopub.status.idle":"2025-02-07T14:13:28.323006Z","shell.execute_reply.started":"2025-02-07T14:13:28.317058Z","shell.execute_reply":"2025-02-07T14:13:28.322036Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Paths\ntrain_path = '/kaggle/input/capsule-vision/archive/Dataset/training'\nval_path = '/kaggle/input/capsule-vision/archive/Dataset/validation'\ntest_path = '/kaggle/input/capvis-test/Test set with seperated folders of each class label'\n\nrandom_seed = np.random.seed(1142)\n# Hyperparameters\nbatch_size = 16\nepochs = 10\nlearning_rate = 1e-6\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Image transformations\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),  # Resize images to match model input\n        transforms.RandomResizedCrop(224),  # Random resized crop for training\n        transforms.RandomHorizontalFlip(),  # Random horizontal flip for augmentation\n        transforms.ToTensor(),  # Convert to PyTorch tensor\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize to ImageNet stats\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((224, 224)),  # Resize images to match model input\n        transforms.CenterCrop(224),  # Center crop for testing\n        transforms.ToTensor(),  # Convert to PyTorch tensor\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize to ImageNet stats\n    ]),\n}\n\n# Data loaders with the new transformations\ntrain_dataset = ImageFolder(root=train_path, transform=data_transforms['train'])\nval_dataset = ImageFolder(root=val_path, transform=data_transforms['test'])\ntest_dataset = ImageFolder(root=test_path, transform=data_transforms['test'])\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:15:52.421399Z","iopub.execute_input":"2025-02-07T14:15:52.422101Z","iopub.status.idle":"2025-02-07T14:16:48.520732Z","shell.execute_reply.started":"2025-02-07T14:15:52.422068Z","shell.execute_reply":"2025-02-07T14:16:48.519985Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Load and initialize DinoV2 model","metadata":{}},{"cell_type":"code","source":"# Load DINO model\ndinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n\n# Define the custom model\nclass DinoVisionTransformerClassifier(nn.Module):\n    def __init__(self):\n        super(DinoVisionTransformerClassifier, self).__init__()\n        self.transformer = dinov2_vits14\n        self.classifier = nn.Sequential(\n            nn.Linear(384, 256),\n            nn.ReLU(),\n            nn.Linear(256, 10)  # Change to the number of classes in your dataset\n        )\n    \n    def forward(self, x):\n        x = self.transformer(x)  # Extract features\n        x = self.transformer.norm(x)  # Normalize the features\n        x = self.classifier(x)  # Classify\n        return x\n\n# Initialize model, loss, and optimizer\ndino_model = DinoVisionTransformerClassifier()\ndino_model = dino_model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(dino_model.parameters(), lr=learning_rate)\n\ncheckpoint = torch.load('/kaggle/input/dinov2_capvis/pytorch/default/1/dino_vit_classifier_capsule.pth')\ndino_model.load_state_dict(checkpoint)  # Load weights into the model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and initialize Hiera model","metadata":{}},{"cell_type":"code","source":"# Load Hiera model\nhiera_base = torch.hub.load(\"facebookresearch/hiera\", model=\"hiera_tiny_224\", pretrained=True, checkpoint=\"mae_in1k_ft_in1k\")\n\n# Define the custom model\nclass HieraVisionTransformerClassifier(nn.Module):\n    def __init__(self):\n        super(HieraVisionTransformerClassifier, self).__init__()\n        self.transformer = hiera_base\n        self.layer_norm = nn.LayerNorm(1000)  # Normalize the 1000 features output from Hiera model\n        self.classifier = nn.Sequential(\n            nn.Linear(1000, 256),\n            nn.ReLU(),\n            nn.Linear(256, 10)  # Change to the number of classes in your dataset\n        )\n    \n    def forward(self, x):\n        x = self.transformer(x)  # Extract features\n        x = self.layer_norm(x)  # Normalize the features\n        x = self.classifier(x)\n        return x\n\n# Initialize model, loss, and optimizer\nhiera_model = HieraVisionTransformerClassifier()\nhiera_model = hiera_model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(hiera_model.parameters(), lr=learning_rate)\n\ncheckpoint = torch.load('/kaggle/input/hierav3/pytorch/default/1/Hiera_model_3.pth')\nmodel.load_state_dict(checkpoint)  # Load weights into the model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and initialize AimV2 model","metadata":{}},{"cell_type":"code","source":"# Load AimV2 model\nclass AIMv2VisionTransformerClassifier(nn.Module):\n    def __init__(self, model_name=\"aimv2-large-patch14-224\", img_size=224, num_classes=10):\n        super(AIMv2VisionTransformerClassifier, self).__init__()\n        \n        # Load AIMv2 pretrained model\n        self.transformer = load_pretrained(model_name, backend=\"torch\")\n        self.pooling = nn.AdaptiveAvgPool1d(1)  # To perform global average pooling\n        # Define a classifier on top of AIMv2 features\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, 256),  # Use AIMv2's feature dimension\n            nn.ReLU(),\n            nn.Linear(256, num_classes)  # Adjust for the number of dataset classes\n        )\n    \n    def forward(self, x):\n        features = self.transformer(x)  # Extract features from AIMv2\n        features = features.permute(0, 2, 1)\n        # Use adaptive average pooling to reduce patch dimension\n        features = self.pooling(features).squeeze(-1)\n        output = self.classifier(features)\n        return output\n\nmodel_name = \"aimv2-large-patch14-224\"\naim_model = AIMv2VisionTransformerClassifier(model_name=model_name)\n\n# Move the model to the device (e.g., GPU or CPU)\naim_model = aim_model.to(device)\n\n# Initialize loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(aim_model.parameters(), lr=learning_rate)\n\ncheckpoint = torch.load('/kaggle/input/aimv2/pytorch/default/1/aimv2_model_3.pth')\naim_model.load_state_dict(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:32:26.515338Z","iopub.execute_input":"2025-02-07T14:32:26.515739Z","iopub.status.idle":"2025-02-07T14:33:17.818205Z","shell.execute_reply.started":"2025-02-07T14:32:26.515706Z","shell.execute_reply":"2025-02-07T14:33:17.817245Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61382ee4c90b44ce938603c6062e8c0e"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    # Focal loss function to address class imbalance by focusing on hard-to-classify examples.\n    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha  # Weighting factor for class imbalance\n        self.gamma = gamma  # Focusing parameter\n        self.reduction = reduction  # How to reduce the final loss\n\n    def forward(self, inputs, targets):\n        # Apply softmax to get probabilities for each class\n        probs = F.softmax(inputs, dim=1)        \n        # Get the probability of the true class\n        p_t = probs.gather(1, targets.view(-1, 1))  # Shape (N, 1)        \n        # Compute the focal loss part (1 - p_t)^gamma\n        loss = -self.alpha * (1 - p_t) ** self.gamma * torch.log(p_t)       \n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:  # 'none'\n            return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:17:18.223886Z","iopub.execute_input":"2025-02-07T14:17:18.224280Z","iopub.status.idle":"2025-02-07T14:17:18.234535Z","shell.execute_reply.started":"2025-02-07T14:17:18.224224Z","shell.execute_reply":"2025-02-07T14:17:18.232748Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Train and test model","metadata":{}},{"cell_type":"code","source":"# Helper functions to train, test and validate model\n\n# Training loop with running loss\ndef train_model(epoch, model):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Using tqdm to show progress\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\", leave=True)\n    for batch_idx, (images, labels) in enumerate(progress_bar):\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n        # Update progress bar description\n        progress_bar.set_postfix(\n            loss=running_loss / (batch_idx + 1),\n            accuracy=100. * correct / total\n        )\n\n# Validation loop with running loss\ndef validate_model(epoch, model):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    # Using tqdm to show progress\n    progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Validation\", leave=True)\n    with torch.no_grad():\n        for batch_idx, (images, labels) in enumerate(progress_bar):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n            # Update progress bar description\n            progress_bar.set_postfix(\n                loss=running_loss / (batch_idx + 1),\n                accuracy=100. * correct / total\n            )\n\n# Testing loop with accuracy\ndef test_model(model):\n    model.eval()\n    correct = 0\n    total = 0\n    all_probabilities = []  # To store all probabilities\n    all_predictions = []    # To store all predicted classes\n    all_labels = []         # To store all true labels for reference\n\n    with torch.no_grad():\n        for batch_idx, (images, labels) in enumerate(tqdm(val_loader, desc=\"Testing\", leave=True)):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            \n            # Calculate probabilities\n            probabilities = F.softmax(outputs, dim=1)\n            \n            # Get the predicted classes\n            _, predicted = outputs.max(1)\n            \n            # Update statistics\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            # Store probabilities, predictions, and labels\n            all_probabilities.append(probabilities.cpu())\n            all_predictions.append(predicted.cpu())\n            all_labels.append(labels.cpu())\n\n    # Concatenate all batches for a single tensor\n    all_probabilities = torch.cat(all_probabilities)\n    all_predictions = torch.cat(all_predictions)\n    all_labels = torch.cat(all_labels)\n\n    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n    \n    return all_probabilities, all_predictions, all_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:19:51.115137Z","iopub.execute_input":"2025-02-07T14:19:51.115514Z","iopub.status.idle":"2025-02-07T14:19:51.127039Z","shell.execute_reply.started":"2025-02-07T14:19:51.115484Z","shell.execute_reply":"2025-02-07T14:19:51.126094Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Training loop\nepochs = 3\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n    train_model(epoch, aim_model)  # Train model for one epoch\n    validate_model(epoch, aim_model)  # Validate model after training\n    print(\"-\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Save model after training\ntorch.save(aim_model.state_dict(), 'aimv2_model_5.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing the model\nprob, pred, labels = test_model(aim_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Functions for evaluation (from organizers)\ndef save_predictions_to_excel(image_paths, y_pred, output_path):\n    \"\"\"\n    Saves predictions along with their probabilities and image paths to an Excel file.\n\n    Parameters:\n    - image_paths: List of image file paths.\n    - y_pred: Array of predicted class probabilities (shape: [n_samples, n_classes]).\n    - output_path: File path for saving the Excel file.\n    \"\"\"\n    \n    class_columns = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', 'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    predicted_class_names = [class_columns[i] for i in y_pred_classes]\n    \n    df_prob = pd.DataFrame(y_pred, columns=class_columns)\n    df_prob.insert(0, 'image_path', image_paths)\n    df_class = pd.DataFrame({'image_path': image_paths, 'predicted_class': predicted_class_names})\n    \n    df_merged = pd.merge(df_prob, df_class, on='image_path')\n    df_merged.to_excel(output_path, index=False)\n\ndef calculate_specificity(y_true, y_pred):\n    \"\"\"\n    Calculates specificity: TN / (TN + FP).\n    \n    Parameters:\n    - y_true: Ground truth binary labels (0 or 1).\n    - y_pred: Predicted binary labels (0 or 1).\n    \n    Returns:\n    - specificity: Specificity score, or 0 if denominator is zero.\n    \"\"\"\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    return specificity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:17:32.031450Z","iopub.execute_input":"2025-02-07T14:17:32.032123Z","iopub.status.idle":"2025-02-07T14:17:32.038710Z","shell.execute_reply.started":"2025-02-07T14:17:32.032089Z","shell.execute_reply":"2025-02-07T14:17:32.037766Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def generate_metrics_report(y_true, y_pred):\n    \"\"\"\n    Generates a comprehensive metrics report for a multi-class classification problem.\n    \n    Parameters:\n    - y_true: Ground truth labels, one-hot encoded (numpy array of shape (n_samples, n_classes)).\n    - y_pred: Predicted probabilities for each class (numpy array of shape (n_samples, n_classes)).\n    \n    Returns:\n    - metrics_report: A JSON string containing various performance metrics including AUC-ROC, \n                      specificity, average precision, sensitivity, F1-score, and balanced accuracy.\n    \"\"\"\n    class_columns = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', 'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\n    metrics_report = {}\n\n    y_true_classes = np.argmax(y_true, axis=1)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n\n    class_report = classification_report(y_true_classes, y_pred_classes, target_names=class_columns, output_dict=True, zero_division=0)\n\n    auc_roc_scores = {class_name: roc_auc_score(y_true[:, i], y_pred[:, i]) for i, class_name in enumerate(class_columns)}\n    mean_auc_roc = np.mean(list(auc_roc_scores.values()))\n    auc_roc_scores['mean_auc'] = mean_auc_roc\n\n    specificity_scores = {class_name: calculate_specificity(y_true[:, i], (y_pred[:, i] >= 0.5).astype(int)) for i, class_name in enumerate(class_columns)}\n    mean_specificity = np.mean(list(specificity_scores.values()))\n    specificity_scores['mean_specificity'] = mean_specificity\n\n    average_precision_scores = {}\n    for i, class_name in enumerate(class_columns):\n        precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred[:, i])\n        average_precision_scores[class_name] = auc(recall, precision) if len(precision) > 0 else 0.0\n    mean_average_precision = np.mean(list(average_precision_scores.values()))\n    average_precision_scores['mean_average_precision'] = mean_average_precision\n\n    sensitivity_scores = {class_name: recall_score(y_true[:, i], (y_pred[:, i] >= 0.5).astype(int), zero_division=0) for i, class_name in enumerate(class_columns)}\n    mean_sensitivity = np.mean(list(sensitivity_scores.values()))\n    sensitivity_scores['mean_sensitivity'] = mean_sensitivity\n\n    f1_scores = {class_name: f1_score(y_true[:, i], (y_pred[:, i] >= 0.5).astype(int), zero_division=0) for i, class_name in enumerate(class_columns)}\n    mean_f1_score = np.mean(list(f1_scores.values()))\n    f1_scores['mean_f1_score'] = mean_f1_score\n\n    balanced_accuracy = balanced_accuracy_score(y_true_classes, y_pred_classes)\n\n    metrics_report.update(class_report)\n    metrics_report['auc_roc_scores'] = auc_roc_scores\n    metrics_report['specificity_scores'] = specificity_scores\n    metrics_report['average_precision_scores'] = average_precision_scores\n    metrics_report['sensitivity_scores'] = sensitivity_scores\n    metrics_report['f1_scores'] = f1_scores\n    metrics_report['mean_auc'] = mean_auc_roc\n    metrics_report['mean_specificity'] = mean_specificity\n    metrics_report['mean_average_precision'] = mean_average_precision\n    metrics_report['mean_sensitivity'] = mean_sensitivity\n    metrics_report['mean_f1_score'] = mean_f1_score\n    metrics_report['balanced_accuracy'] = balanced_accuracy\n\n    return json.dumps(metrics_report, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:17:34.787577Z","iopub.execute_input":"2025-02-07T14:17:34.788422Z","iopub.status.idle":"2025-02-07T14:17:34.799040Z","shell.execute_reply.started":"2025-02-07T14:17:34.788386Z","shell.execute_reply":"2025-02-07T14:17:34.798132Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=10)\n# Generate and save evaluation metrics as JSON\nmetrics_report = generate_metrics_report(np.array(one_hot_labels), np.array(prob))\nmetrics_report_path = '/kaggle/working/metrics_report.json'\nwith open(metrics_report_path, 'w') as f:\n    f.write(metrics_report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:42:40.894277Z","iopub.execute_input":"2025-02-07T14:42:40.894933Z","iopub.status.idle":"2025-02-07T14:42:41.170732Z","shell.execute_reply.started":"2025-02-07T14:42:40.894902Z","shell.execute_reply":"2025-02-07T14:42:41.170009Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score\n\n# Compute metrics\naccuracy = accuracy_score(labels, pred)\nbalanced_acc = balanced_accuracy_score(labels, pred)\nclass_report = classification_report(labels, pred, target_names=test_loader.dataset.classes, zero_division=1)\n\n# Print metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Balanced Accuracy: {balanced_acc:.4f}\")\nprint(\"Classification Report:\")\nprint(class_report)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', 'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\ny_pred = prob\ny_pred_classes = pred\n#  Get true labels from the validation generator\ny_val = labels # Assuming classes are set in the generator\ny_val_tensor = torch.tensor(y_val)  # Convert to a tensor if not already\ny_val_one_hot = torch.nn.functional.one_hot(y_val_tensor, num_classes=10)\nclass_names = list(test_loader.dataset.classes)\n\n# Convert the class indices back to class names for both actual and predicted values\nactual_class_names = [class_names[i] for i in y_val]\npredicted_class_names = [class_names[i] for i in y_pred_classes]\n\n#Create a DataFrame with the predicted probabilities for each class (y_pred)\ndf_predictions = pd.DataFrame(y_pred, columns=class_names)\nimage_paths = image_paths = [test_loader.dataset.samples[i][0] for i in range(len(test_loader.dataset))]\n\n# Add the image paths, actual class, and predicted class to the DataFrame\ndf_predictions.insert(0, 'image_path', image_paths)  # Insert image names as the first column\ndf_predictions['predicted_class'] = predicted_class_names  # Add predicted class as a column\ndf_predictions['actual_class'] = actual_class_names  # Add actual class as a column\n\n# Save to Excel\noutput_path = '/kaggle/working/test_results.xlsx'\ndf_predictions.to_excel(output_path, index=False)\n\n# print(f\"Validation results with probabilities saved to {output_path}\")\n\n# Plot AUC-ROC curve for each class and save it as PNG\nlb = LabelBinarizer()\ny_val_bin = lb.fit_transform(y_val)\nfpr, tpr, roc_auc = {}, {}, {}\n\nfor i in range(len(class_names)):\n    fpr[i], tpr[i], _ = roc_curve(y_val_bin[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure()\nfor i in range(len(class_names)):\n    plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AUC-ROC Curve')\nplt.legend(loc='lower right')\nroc_curve_path = '/kaggle/working/roc_curve.pdf'\nplt.savefig(roc_curve_path, format='pdf')  # Save the AUC-ROC curve plot as PNG\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print Confusion Matrix\nprint(\"Confusion Matrix:\")\ncm = confusion_matrix(y_val, y_pred_classes, normalize='true')\nprint(cm)\n\n# Print Classification Report\nprint(\"Classification Report:\")\nprint(classification_report(y_val, y_pred_classes, target_names=class_names))\n\n# Plot normalized Confusion Matrix and save it as PNG\nplt.figure(figsize=(12, 12))\nsns.heatmap(cm, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Normalized Confusion Matrix')\nplt.tight_layout()\nconf_matrix_path = '/kaggle/working/confusion_matrix.pdf'\nplt.savefig(conf_matrix_path, format='pdf')  # Save the confusion matrix plot as PNG\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Calculate number of parameters and Flops","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dummy input tensor (batch size of 1 and input size (224, 224, 3))\ndummy_input = torch.randn(1, 3, 224, 224).to(device)\n\n# Calculate number of parameters\nnum_params = sum(p.numel() for p in dino_model.parameters())\nprint(f\"Number of parameters: {num_params:,}\")\n\n# Calculate FLOPs using thop\nflops, _ = profile(dino_model, inputs=(dummy_input,))\nprint(f\"FLOPs: {flops:,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate number of parameters\nnum_params = sum(p.numel() for p in aim_model.parameters())\nprint(f\"Number of parameters: {num_params:,}\")\n\n# Calculate FLOPs using thop\nflops, _ = profile(aim_model, inputs=(dummy_input,))\nprint(f\"FLOPs: {flops:,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate number of parameters\nnum_params = sum(p.numel() for p in hiera_model.parameters())\nprint(f\"Number of parameters: {num_params:,}\")\n\n# Calculate FLOPs using thop\nflops, _ = profile(hiera_model, inputs=(dummy_input,))\nprint(f\"FLOPs: {flops:,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}